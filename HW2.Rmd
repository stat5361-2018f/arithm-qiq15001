---
title: "HW2"
author: "Qi Qi"
date: "9/14/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Exercise 1 
\section{Approximation of distribution function of standard normal}

\subsection{Abstract}
Monte Carlo methods have been used to approximate the distribution function of standard normal. Box plots of bias are also established. After the comparison, I conclude that when sample size is enlarged, the approximations will get close to the true values.


\subsection{Method}
The distribution function of $N(0,1)$ is

$$ \Phi(t)=\int_{-\infty}^t\frac{1}{\sqrt{2\pi}}e^{-y^2/2}dy$$

I compute the true values at $t\in\{0.0, 0.67, 0.84, 1.28, 1.65, 2.32, 2.58, 3.09, 3.72\}$.

To calculate the approximated values, I generate random samples with sample size $n\in\{10^2, 10^3, 10^4\}$ and evaluate the approximation at all values of $t$ by Monte Carlo method:

$$\hat{\Phi}(t)=\frac{1}{n}\sum_{i=1}^{n}I(X_i\leq t)$$



\subsection{Comparison of true values and approximations}
I use `pnorm` function to calculate the true values of distribution function at all the values of $t$ and then compute the approximations by Monte Carlo method described previously. Table 1 is generated and contains $t$, true values and approximations at different sample size n.
```{r}
t<- c(0.0, 0.67, 0.84, 1.28, 1.65, 2.32, 2.58, 3.09, 3.72)
true_value<- pnorm(t, 0, 1)
apprx1<- NA
apprx2<- NA
apprx3<- NA
# n=10^2
x<- rnorm(100, 0, 1)
for (i in 1:length(t)){
  apprx1[i]<- sum(x<=t[i])/100
}

# n=10^3
y<- rnorm(1000, 0, 1)
for (i in 1:length(t)){
  apprx2[i]<- sum(y<=t[i])/1000
}

# n=10^4
z<- rnorm(10000, 0, 1)
for (i in 1:length(t)){
  apprx3[i]<- sum(z<=t[i])/10000
}

tab<- cbind(t, true_value, apprx1, apprx2, apprx3)
knitr::kable(tab, col.names = c("t", "True value", "Approx at n=10^2", "Approx at n=10^3", "Approx at n=10^4"), caption = "Comparison of true values and approximation")
```

Further, I repeat the experiment 100 times and create box plots of bias at all $t$.

```{r}
library(ggplot2)
bias1<- matrix(NA, nrow = length(t), ncol = 100)
bias2<- matrix(NA, nrow = length(t), ncol = 100)
bias3<- matrix(NA, nrow = length(t), ncol = 100)
for (i in 1:length(t)){
  for (j in 1:100){
    x<- rnorm(100, 0, 1)
    bias1[i, j] <- sum(x<=t[i])/100- true_value[i]
    y<- rnorm(1000, 0, 1)
    bias2[i, j]<- sum(y<=t[i])/1000- true_value[i]
    z<- rnorm(10000, 0, 1)
    bias3[i, j]<- sum(z<=t[i])/10000- true_value[i]
  }
  
}

ggplot(stack(data.frame(t(bias1))), aes(x=ind, y=values))+
  geom_boxplot()+
  scale_x_discrete(labels=c(0.0, 0.67, 0.84, 1.28, 1.65, 2.32, 2.58, 3.09, 3.72), name="t")+
  scale_y_continuous(name="bias")+
  ggtitle("Figure 1: bias at n=10^2")+
  theme(plot.title = element_text(hjust = 0.5))

ggplot(stack(data.frame(t(bias2))), aes(x=ind, y=values))+
  geom_boxplot()+
  scale_x_discrete(labels=c(0.0, 0.67, 0.84, 1.28, 1.65, 2.32, 2.58, 3.09, 3.72), name="t")+
  scale_y_continuous(name="bias")+
  ggtitle("Figure 2: bias at n=10^3")+
  theme(plot.title = element_text(hjust = 0.5))

ggplot(stack(data.frame(t(bias3))), aes(x=ind, y=values))+
  geom_boxplot()+
  scale_x_discrete(labels=c(0.0, 0.67, 0.84, 1.28, 1.65, 2.32, 2.58, 3.09, 3.72), name="t")+
  scale_y_continuous(name="bias")+
  ggtitle("Figure 3: bias at n=10^4")+
  theme(plot.title = element_text(hjust = 0.5))

```


\subsection{Conclusion}
According to Table 1, we can notice that when $n=10^4$ the approximations are most close to the true values. After repeating experiment 100 times, we can state the bias are more close to 0 with larger $n$ at each value of $t$ from Figue 1, 2 and 3. On the other hand, from each figure, we can find that with same sample size $n$ bias will be closer to 0 at larger value of $t$. Thus, this indicate that enlarging sample size will reduce the error of approximation and it will be more accurate if we approximate at larger $t$.

# Exercise 2
`.Machine$double.xmax` is the largest normalized floating-point number. Normally, it is $1.797693e+308$.

` .Machine$double.xmin` is the smallest non-zero normalized floating-point number. Normally, it is $2.225074e-308$.

`.Machine$double.eps` is the smallest positive floating-point number x such that $1 + x != 1$. Normally, it is $2.220446e-16$.

`.Machine$double.neg.eps` is a small positive floating-point number x such that $1 - x != 1$. Normally, it is $1.110223e-16$.
